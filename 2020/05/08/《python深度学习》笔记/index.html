<!DOCTYPE html>


<html lang="zh-CN" >


<head>
  <meta charset="utf-8" />
   
  <meta name="keywords" content="技术,旅行，小文艺" />
   
  <meta name="description" content="博客随笔" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    《python深度学习》笔记 |  Welcome to Mr.l&#39;s World
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

<link rel="alternate" href="/atom.xml" title="Welcome to Mr.l's World" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    <main class="content on">
      <section class="outer">
  <article id="post-《python深度学习》笔记" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  《python深度学习》笔记
</h1>
 

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/05/08/%E3%80%8Apython%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-05-08T01:41:00.000Z" itemprop="datePublished">2020-05-08</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/">技术笔记</a> / <a class="article-category-link" href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E3%80%8Apython%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/">《python深度学习》</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">5.9k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">21 分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="第一章-什么是深度学习"><a href="#第一章-什么是深度学习" class="headerlink" title="第一章 什么是深度学习"></a>第一章 什么是深度学习</h1><h2 id="1-1-人工智能、机器学习与深度学习"><a href="#1-1-人工智能、机器学习与深度学习" class="headerlink" title="1.1　人工智能、机器学习与深度学习"></a><strong>1.1　人工智能、机器学习与深度学习</strong></h2><blockquote>
<p>人工智能、机器学习与深度学习这三者之间的关系：{深度学习}&lt;{机器学习}&lt;{人工智能}</p>
</blockquote>
<h3 id="1-1-1-人工智能"><a href="#1-1-1-人工智能" class="headerlink" title="1.1.1　人工智能"></a>1.1.1　人工智能</h3><ul>
<li>人工智能的简洁定义如下：<strong>努力将通常由人类完成的智力任务自动化。</strong>因此，<strong>人工智能是一个综合性的领域，不仅包括机器学习与深度学习，还包括更多不涉及学习的方法</strong>。</li>
</ul>
<h3 id="1-1-2-机器学习"><a href="#1-1-2-机器学习" class="headerlink" title="1.1.2　机器学习"></a>1.1.2　机器学习</h3><ul>
<li><p>在经典的程序设计（即符号主义人工智能的范式）中，人们<strong>输入的是规则（即程序）</strong>和需要根据这些规则<strong>进行处理的数据</strong>，系统<strong>输出的是答案</strong>。</p>
</li>
<li><p>在机器学习中，人们<strong>输入的是数据</strong>和从这些数据中<strong>预期得到的答案</strong>，系统<strong>输出的是规则</strong>。这些规则随后可应用于新的数据，并使计算机自主生成答案。</p>
</li>
<li><p>机器学习系统是<strong>训练出来的</strong>，而不是明确地用程序编写出来的。</p>
</li>
<li><p>机器学习（尤其是深度学习）呈现出相对较少的数学理论（可能太少了），并且是以工程为导向的。这是一门<strong>需要上手实践的学科</strong>，想法<strong>更多地是靠实践来证明</strong>，而不是靠理论推导。</p>
</li>
</ul>
<a id="more"></a>
<h3 id="1-1-3-从数据中学习表示"><a href="#1-1-3-从数据中学习表示" class="headerlink" title="1.1.3　从数据中学习表示"></a>1.1.3　从数据中学习表示</h3><ul>
<li><p>机器学习和深度学习的核心问题在于<strong>有意义地变换数据</strong>，换句话说，在于学习输入数据的<strong>有用表示（representation）</strong>——这种表示可以让数据更接近预期输出。</p>
</li>
<li><p>什么是<strong>表示</strong>：这一概念的核心在于<strong>以一种不同的方式来查看数据（即表征数据或将数据编码）</strong>。例如，彩色图像可以编码为RGB（红-绿-蓝）格式或HSV（色相-饱和度-明度）格式，这是对相同数据的两种不同表示。在处理某些任务时，使用某种表示可能会很困难，但换用另一种表示就会变得很简单。</p>
</li>
<li><p>所有机器学习算法都包括<strong>自动寻找这样一种变换</strong>：这种变换可以根据任务将数据转化为更加有用的表示。机器学习算法在寻找这些变换时通常没有什么创造性，而仅仅是遍历一组预先定义好的操作，这组操作叫作<strong>假设空间（hypothesis space）</strong>。</p>
</li>
<li><p>这就是机器学习的技术定义：<strong>在预先定义好的可能性空间中，利用反馈信号的指引来寻找输入数据的有用表示</strong>。</p>
</li>
</ul>
<h3 id="1-1-4-深度学习之“深度”"><a href="#1-1-4-深度学习之“深度”" class="headerlink" title="1.1.4　深度学习之“深度”"></a>1.1.4　深度学习之“深度”</h3><ul>
<li><p>深度学习是机器学习的一个分支领域：它是从数据中学习表示的一种新方法，强调从连续的<strong>层（layer）</strong>中进行学习，这些层对应于<strong>越来越有意义的表示</strong>。</p>
</li>
<li><p>“深度学习”中的“深度”指的并不是利用这种方法所获取的更深层次的理解，而是指<strong>一系列连续的表示层</strong>。数据模型中包含多少层，这被称为模型的<strong>深度（depth）</strong>。这一领域的其他名称包括分层表示学习（layered representations learning）和层级表示学习（hierarchical representations learning）。</p>
</li>
<li><p>现代深度学习通常包含<strong>数十个甚至上百个连续的表示层</strong>，这些表示层全都是从训练数据中自动学习的。与此相反，其他机器学习方法的重点往往是仅仅学习一两层的数据表示，因此有时也被称为<strong>浅层学习（shallow learning）</strong>。</p>
</li>
<li><p>在深度学习中，这些分层表示几乎总是通过叫作<strong>神经网络（neural network）</strong>的模型来学习得到的。</p>
</li>
</ul>
<h3 id="1-1-5-理解深度学习的工作原理"><a href="#1-1-5-理解深度学习的工作原理" class="headerlink" title="1.1.5　理解深度学习的工作原理"></a>1.1.5　理解深度学习的工作原理</h3><ul>
<li><p>神经网络中每层对输入数据所做的具体操作保存在该层的<strong>权重（weight）</strong>中，其本质是一串数字。用术语来说，每层实现的变换由其权重来<strong>参数化（parameterize，见图 1-7）</strong>。权重有时<br>也被称为该层的<strong>参数（parameter）</strong>。在这种语境下，学习的意思是<strong>为神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应</strong>。</p>
</li>
<li><p>想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离。这是神经网络<strong>损失函数（loss function）</strong>的任务，该函数也叫<strong>目标函数（objective function）</strong>。损失函数的输入是网络预测值与真实目标值（即你希望网络输出的结果），然后计算一个距离值，衡量该网络在这个示例上的效果好坏。</p>
</li>
<li><p>深度学习的基本技巧是利用这个距离值作为反馈信号来对权重值进行微调，以降低当前示例对应的损失值。这种调节由<strong>优化器（optimizer）</strong>来完成，它实现了所谓的<strong>反向传播（backpropagation）算法</strong>，这是深度学习的<strong>核心算法</strong>。</p>
</li>
</ul>
<h3 id="1-1-8-人工智能的未来"><a href="#1-1-8-人工智能的未来" class="headerlink" title="1.1.8　人工智能的未来"></a>1.1.8　人工智能的未来</h3><ul>
<li><strong>不要相信短期的炒作，但一定要相信长期的愿景。人工智能可能需要一段时间才能充分发挥其潜力。这一潜力的范围大到难以想象，但人工智能终将到来，它将以一种奇妙的方式改变我们的世界。</strong></li>
</ul>
<h2 id="1-2-深度学习之前：机器学习简史"><a href="#1-2-深度学习之前：机器学习简史" class="headerlink" title="1.2　深度学习之前：机器学习简史"></a><strong>1.2　深度学习之前：机器学习简史</strong></h2><blockquote>
<p>深度学习已经得到了人工智能历史上前所未有的公众关注度和产业投资，但这并不是机器学习的第一次成功。可以这样说，当前工业界所使用的绝大部分机器学习算法都不是深度学习算法。深度学习不一定总是解决问题的正确工具：有时没有足够的数据，深度学习不适用；有时用其他算法可以更好地解决问题。如果你第一次接触的机器学习就是深度学习，那你可能会发现手中握着一把深度学习“锤子”，而所有机器学习问题看起来都像是“钉子”。为了避免陷入这个误区，唯一的方法就是熟悉其他机器学习方法并在适当的时候进行实践。</p>
</blockquote>
<h3 id="1-2-1-概率建模"><a href="#1-2-1-概率建模" class="headerlink" title="1.2.1　概率建模"></a>1.2.1　概率建模</h3><ul>
<li><p><strong>概率建模（probabilistic modeling）</strong>是统计学原理在数据分析中的应用。它是最早的机器学习形式之一，至今仍在广泛使用。</p>
</li>
<li><p>其中最有名的算法之一就是<strong>朴素贝叶斯算法</strong>:它假设输入数据的特征都是独立的。这是一个很强的假设，或者说“朴素的”假设，其名称正来源于此。</p>
</li>
<li><p>另一个密切相关的<strong>分类算法</strong>模型是 <strong>logistic 回归（logistic regression，简称 logreg）</strong>，它有时被认为是现代机器学习的“hello world”。面对一个数据集，数据科学家<strong>通常会首先尝试使用这个算法</strong>，以便初步熟悉手头的分类任务。</p>
</li>
</ul>
<h3 id="1-2-2-早期神经网络"><a href="#1-2-2-早期神经网络" class="headerlink" title="1.2.2　早期神经网络"></a>1.2.2　早期神经网络</h3><ul>
<li>反向传播算法——一种利用梯度下降优化来训练一系列参数化运算链的方法（笔记后面将给出这些概念的具体定义）</li>
</ul>
<h3 id="1-2-3-核方法"><a href="#1-2-3-核方法" class="headerlink" title="1.2.3　核方法"></a>1.2.3　核方法</h3><ul>
<li><p>核方法是一组分类算法，其中最有名的就是<strong>支持向量机（SVM，support vector machine）</strong>。</p>
</li>
<li><p>SVM 的目标是通过在属于两个不同类别的两组数据点之间找到良好<strong>决策边界（decisionboundary）</strong>来解决分类问题。</p>
</li>
<li><p>SVM 通过两步来寻找决策边界:</p>
</li>
</ul>
<ol>
<li>将数据映射到一个新的高维表示，这时决策边界可以用一个超平面来表示（如果数据是二维的，那么超平面就是一条直线）。</li>
<li>尽量让超平面与每个类别最近的数据点之间的<strong>距离最大化</strong>，从而计算出良好决策边界（分割超平面），这一步叫作<strong>间隔最大化（maximizing the margin）</strong>。</li>
</ol>
<ul>
<li><p>将数据映射到高维表示从而使分类问题简化，这一技巧可能听起来很不错，但在实践中通常是难以计算的。这时就需要用到<strong>核技巧（kernel trick，核方法正是因这一核心思想而得名）</strong>。其基本思想是：要想在新的表示空间中找到良好的决策超平面，你不需要在新空间中直接计算点的坐标，只需要在新空间中计算点对之间的距离，而利用<strong>核函数（kernel function）</strong>可以高效地完成这种计算。核函数是一个在计算上能够实现的操作，将原始空间中的任意两点映射为这两点在目标表示空间中的距离，完全避免了对新表示进行直接计算。核函数通常是人为选择的，而不是从数据中学到的。对于 SVM 来说，只有分割超平面是通过学习得到的。</p>
</li>
<li><p>SVM 很难扩展到大型数据集，并且在图像分类等感知问题上的效果也不好。SVM是一种比较浅层的方法，因此要想将其应用于感知问题，首先需要手动提取出有用的表示（这叫作<strong>特征工程</strong>），这一步骤很难，而且不稳定。</p>
</li>
</ul>
<h3 id="1-2-4-决策树、随机森林与梯度提升机"><a href="#1-2-4-决策树、随机森林与梯度提升机" class="headerlink" title="1.2.4　决策树、随机森林与梯度提升机"></a>1.2.4　决策树、随机森林与梯度提升机</h3><ul>
<li><strong>梯度提升方法</strong>可能是目前处理<strong>非感知数据</strong>最好的算法之一（如果非要加个“之一”的话）。</li>
</ul>
<h3 id="1-2-7-机器学习现状"><a href="#1-2-7-机器学习现状" class="headerlink" title="1.2.7　机器学习现状"></a>1.2.7　机器学习现状</h3><ul>
<li>梯度提升机用于<strong>处理结构化数据的问题</strong>，而深度学习则用于<strong>图像分类等感知问题</strong>。使用前一种方法的人几乎都使用优秀的<strong>XGBoost库</strong>，它同时支持数据科学最流行的两种语言：Python 和 R。使用深度学习的 Kaggle 参赛者则大多使用 <strong>Keras 库</strong>，因为它易于使用，非常灵活，并且支持Python。</li>
</ul>
<h2 id="1-3-为什么是深度学习，为什么是现在"><a href="#1-3-为什么是深度学习，为什么是现在" class="headerlink" title="1.3　为什么是深度学习，为什么是现在"></a><strong>1.3　为什么是深度学习，为什么是现在</strong></h2><blockquote>
<p>由于这一领域是靠实验结果而不是理论指导的，所以只有当合适的数据和硬件可用于尝试新想法时（或者将旧想法的规模扩大，事实往往也是如此），才可能出现算法上的改进。<strong>机器学习不是数学或物理学，靠一支笔和一张纸就能实现重大进展。它是一门工程科学</strong>。在 20 世纪 90 年代和 21 世纪前十年，真正的瓶颈在于数据和硬件。但在这段时间内发生了下面这些事情：互联网高速发展，并且针对游戏市场的需求开发出了高性能图形芯片。</p>
</blockquote>
<hr>
<h1 id="第二章-神经网络的数学基础"><a href="#第二章-神经网络的数学基础" class="headerlink" title="第二章 神经网络的数学基础"></a>第二章 神经网络的数学基础</h1><h2 id="2-2-神经网络的数据表示"><a href="#2-2-神经网络的数据表示" class="headerlink" title="2.2　神经网络的数据表示"></a><strong>2.2　神经网络的数据表示</strong></h2><blockquote>
<p>数据存储在多维 Numpy 数组中，也叫<strong>张量（tensor）</strong>。矩阵是二维张量,张量是矩阵向任意维度的推广［注意:张量的<strong>维度（dimension）</strong>通常叫作<strong>轴（axis）</strong>］。</p>
</blockquote>
<h3 id="2-2-1-标量（0D-张量）"><a href="#2-2-1-标量（0D-张量）" class="headerlink" title="2.2.1　标量（0D 张量）"></a>2.2.1　标量（0D 张量）</h3><ul>
<li><p>仅包含一个数字的张量叫作<strong>标量</strong>（scalar，也叫<strong>标量张量、零维张量、0D 张量</strong>）。</p>
</li>
<li><p>你可以用 ndim 属性来查看一个 Numpy 张量的轴的个数。标量张量有 0 个轴（ndim == 0）。张量轴的个数也叫作<strong>阶（rank）</strong>。下面展示一个 Numpy 标量。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; x = np.array(12)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">array(12)</span><br><span class="line">&gt;&gt;&gt; x.ndim</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="2-2-2-向量（1D-张量）"><a href="#2-2-2-向量（1D-张量）" class="headerlink" title="2.2.2　向量（1D 张量）"></a>2.2.2　向量（1D 张量）</h3><ul>
<li><p>数字组成的数组叫作<strong>向量（vector）</strong>或<strong>一维张量（1D 张量）</strong>。一维张量只有一个轴,下面是一个 Numpy 向量。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = np.array([12, 3, 6, 14, 7])</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">array([12, 3, 6, 14, 7])</span><br><span class="line">&gt;&gt;&gt; x.ndim</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
</li>
<li><p>这个<strong>向量</strong>有 5 个元素，所以被称为 <strong>5D 向量</strong>。不要把<strong>5D 向量</strong>和 <strong><em>5D 张量</em></strong>弄混！ <strong>5D 向量</strong>只有一个轴，沿着轴有 5 个维度，而 <strong><em>5D 张量</em></strong>有 5 个轴（沿着每个轴可能有任意个维度）。<strong>维度（dimensionality）</strong>可以表示沿着某个轴上的元素个数（比如 5D 向量），也可以表示张量中轴的个数（比如 5D 张量），这有时会令人感到混乱。对于后一种情况，技术上更准确的说法是 <strong>5 阶张量</strong>（张量的阶数即轴的个数），但 5D 张量这种模糊的写法更常见。</p>
</li>
</ul>
<h3 id="2-2-3-矩阵（2D-张量）"><a href="#2-2-3-矩阵（2D-张量）" class="headerlink" title="2.2.3　矩阵（2D 张量）"></a>2.2.3　矩阵（2D 张量）</h3><ul>
<li>向量组成的数组叫作<strong>矩阵（matrix）</strong>或<strong>二维张量（2D 张量）</strong>。矩阵有 <strong>2 个轴</strong>（通常叫作<strong>行</strong>和<strong>列</strong>）。你可以将矩阵直观地理解为数字组成的矩形网格。下面是一个 Numpy 矩阵.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = np.array([[5, 78, 2, 34, 0],</span><br><span class="line"> [6, 79, 3, 35, 1],</span><br><span class="line"> [7, 80, 4, 36, 2]])</span><br><span class="line">&gt;&gt;&gt; x.ndim</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
第一个轴上的元素叫作行（row），第二个轴上的元素叫作列（column）。在上面的例子中，[5, 78, 2, 34, 0] 是 x 的第一行，[5, 6, 7] 是第一列。</li>
</ul>
<h3 id="2-2-4-3D-张量与更高维张量"><a href="#2-2-4-3D-张量与更高维张量" class="headerlink" title="2.2.4 3D 张量与更高维张量"></a>2.2.4 3D 张量与更高维张量</h3><ul>
<li>将多个矩阵组合成一个新的数组，可以得到一个 3D 张量，你可以将其直观地理解为数字组成的立方体。下面是一个 Numpy 的 3D 张量。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = np.array([[[5, 78, 2, 34, 0],</span><br><span class="line"> [6, 79, 3, 35, 1],</span><br><span class="line"> [7, 80, 4, 36, 2]],</span><br><span class="line"> [[5, 78, 2, 34, 0],</span><br><span class="line"> [6, 79, 3, 35, 1],</span><br><span class="line"> [7, 80, 4, 36, 2]],</span><br><span class="line"> [[5, 78, 2, 34, 0],</span><br><span class="line"> [6, 79, 3, 35, 1],</span><br><span class="line"> [7, 80, 4, 36, 2]]])</span><br><span class="line">&gt;&gt;&gt; x.ndim</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
将多个 3D 张量组合成一个数组，可以创建一个 4D 张量，以此类推。深度学习处理的一般是 0D 到 4D 的张量，但处理视频数据时可能会遇到 5D 张量。</li>
</ul>
<h3 id="2-2-5-关键属性"><a href="#2-2-5-关键属性" class="headerlink" title="2.2.5　关键属性"></a>2.2.5　关键属性</h3><ul>
<li>张量是由以下三个关键属性来定义的:</li>
</ul>
<ol>
<li><p><strong>轴的个数（阶）</strong>。例如，3D 张量有 3 个轴，矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim(轴)。</p>
</li>
<li><p><strong>形状</strong>。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。例如，前面矩阵示例的形状为 (3, 5)，3D 张量示例的形状为 (3, 3, 5)。向量的形状只包含一个元素，比如 (5,)，而标量的形状为空，即 ()。</p>
</li>
<li><p><strong>数据类型</strong>（在 Python 库中通常叫作 dtype）。这是张量中所包含数据的类型，例如，张量的类型可以是 float32、uint8、float64 等。在极少数情况下，你可能会遇到字符（char）张量。<strong><em>注意</em></strong>:Numpy（以及大多数其他库）中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。</p>
</li>
</ol>
<h3 id="2-2-7-数据批量的概念"><a href="#2-2-7-数据批量的概念" class="headerlink" title="2.2.7　数据批量的概念"></a>2.2.7　数据批量的概念</h3><ul>
<li><p>通常来说，深度学习中所有数据张量的第一个轴（0 轴，因为索引从 0 开始）都是<strong>样本轴（samples axis，有时也叫样本维度）</strong>。在 MNIST 的例子中，样本就是数字图像。</p>
</li>
<li><p>此外，深度学习模型<strong>不会同时处理整个数据集</strong>，而是<strong>将数据拆分成小批量</strong>。具体来看，下面是 MNIST 数据集的一个批量，批量大小为 128。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = train_images[:128]</span><br><span class="line"><span class="comment">#然后是下一个批量。</span></span><br><span class="line">batch = train_images[128:256]</span><br><span class="line"><span class="comment">#然后是第 n 个批量。</span></span><br><span class="line">batch = train_images[128 * n:128 * (n + 1)]</span><br></pre></td></tr></table></figure>
<p>对于这种批量张量，第一个轴（0 轴）叫作<strong>批量轴（batch axis）</strong>或<strong>批量维度（batch dimension）</strong>。在使用 Keras 和其他深度学习库时会经常遇到这个术语。</p>
</li>
</ul>
<h3 id="2-2-8-现实世界中的数据张量"><a href="#2-2-8-现实世界中的数据张量" class="headerlink" title="2.2.8　现实世界中的数据张量"></a>2.2.8　现实世界中的数据张量</h3><ul>
<li>我们用几个你未来会遇到的示例来具体介绍数据张量。你需要处理的数据几乎总是以下类别之一:</li>
</ul>
<ol>
<li><p><strong>向量数据</strong>：2D 张量，形状为 (samples, features)。</p>
</li>
<li><p><strong>时间序列数据或序列数据</strong>：3D 张量，形状为 (samples, timesteps, features)。</p>
</li>
<li><p><strong>图像</strong>：4D 张量，形状为 (samples, height, width, channels) 或 (samples, channels,height, width)。</p>
</li>
<li><p><strong>视频</strong>：5D 张量，形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width)。</p>
</li>
</ol>
<h3 id="2-2-9-向量数据"><a href="#2-2-9-向量数据" class="headerlink" title="2.2.9　向量数据"></a>2.2.9　向量数据</h3><ul>
<li>这是最常见的数据。对于这种数据集，每个数据点都被编码为一个向量，因此一个数据批量就被编码为 2D 张量（即向量组成的数组），其中第一个轴是<strong>样本轴</strong>，第二个轴是<strong>特征轴</strong>。以下展示两个例子:</li>
</ul>
<ol>
<li><p><strong>人口统计数据集</strong>:其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值的向量，而整个数据集包含 100 000 个人，因此可以存储在形状为 (100000, 3) 的 2D张量中。</p>
</li>
<li><p><strong>文本文档数据集</strong>:我们将每个文档表示为每个单词在其中出现的次数（字典中包含20 000 个常见单词）。每个文档可以被编码为包含 20 000 个值的向量（每个值对应于字典中每个单词的出现次数），整个数据集包含 500 个文档，因此可以存储在形状为(500, 20000) 的张量中。</p>
</li>
</ol>
<h3 id="2-2-10-时间序列数据或序列数据"><a href="#2-2-10-时间序列数据或序列数据" class="headerlink" title="2.2.10　时间序列数据或序列数据"></a>2.2.10　时间序列数据或序列数据</h3><ul>
<li>当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的 3D 张量中。每个样本可以被编码为一个向量序列（即 2D 张量），因此一个数据批量就被编码为一个 3D 张量。根据惯例，时间轴始终是第 2 个轴（索引为 1 的轴）。以下展示两个例子:</li>
</ul>
<ol>
<li><p><strong>股票价格数据集</strong>。每一分钟，我们将股票的当前价格、前一分钟的最高价格和前一分钟的最低价格保存下来。因此每分钟被编码为一个 3D 向量，整个交易日被编码为一个形状为 (390, 3) 的 2D 张量（一个交易日有 390 分钟），而 250 天的数据则可以保存在一个形状为 (250, 390, 3) 的 3D 张量中。这里每个样本是一天的股票数据。</p>
</li>
<li><p><strong>推文数据集</strong>。我们将每条推文编码为 280 个字符组成的序列，而每个字符又来自于 128个字符组成的字母表。在这种情况下，每个字符可以被编码为大小为 128 的二进制向量（只有在该字符对应的索引位置取值为 1，其他元素都为 0）。那么每条推文可以被编码为一个形状为 (280, 128) 的 2D 张量，而包含 100 万条推文的数据集则可以存储在一个形状为 (1000000, 280, 128) 的张量中。</p>
</li>
</ol>
<h3 id="2-2-11-图像数据"><a href="#2-2-11-图像数据" class="headerlink" title="2.2.11　图像数据"></a>2.2.11　图像数据</h3><ul>
<li><p>图像通常具有三个维度：<strong>高度、宽度和颜色深度</strong>。虽然灰度图像（比如 MNIST 数字图像）只有一个颜色通道，因此可以保存在 2D 张量中，但按照惯例，图像张量始终都是 3D 张量，灰度图像的彩色通道只有一维。因此，如果图像大小为 256×256，那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中，而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中</p>
</li>
<li><p>图像张量的形状有两种约定：<strong>通道在后（channels-last）</strong>的约定（在 TensorFlow 中使用）和<strong>通道在前（channels-first）</strong>的约定（在 Theano 中使用）。Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后：(samples, height, width, color_depth)。与此相反，Theano将图像深度轴放在批量轴之后：(samples, color_depth, height, width)。如果采用 Theano 约定，前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256)。Keras 框架同时支持这两种格式。</p>
</li>
</ul>
<h3 id="2-2-12-视频数据"><a href="#2-2-12-视频数据" class="headerlink" title="2.2.12　视频数据"></a>2.2.12　视频数据</h3><ul>
<li>视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧，每一帧都是一张彩色图像。由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中，因此一系列帧可以保存在一个形状为 (frames, height, width,color_depth) 的 4D 张量中，而不同视频组成的批量则可以保存在一个 5D 张量中，其形状为(samples, frames, height, width, color_depth)。举个例子:</li>
</ul>
<ol>
<li>一个以每秒 4 帧采样的 60 秒 YouTube 视频片段，视频尺寸为 144×256，这个视频共有 240 帧。4 个这样的视频片段组成的批量将保存在形状为 (4, 240, 144, 256, 3)的张量中。总共有 106 168 320 个值！如果张量的数据类型（dtype）是 float32，每个值都是32 位，那么这个张量共有 405MB。好大！你在现实生活中遇到的视频要小得多，因为它们不以float32 格式存储，而且通常被大大压缩，比如 MPEG 格式。</li>
</ol>
<h2 id="2-3-神经网络的“齿轮”：张量运算"><a href="#2-3-神经网络的“齿轮”：张量运算" class="headerlink" title="2.3　神经网络的“齿轮”：张量运算"></a>2.3　神经网络的“齿轮”：张量运算</h2><blockquote>
<p>所有计算机程序最终都可以简化为二进制输入上的一些二进制运算（AND、OR、NOR 等），与此类似，深度神经网络学到的所有变换也都可以简化为数值数据张量上的一些<strong>张量运算（tensor operation）</strong>。Keras 层的实例如下所示:</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(512, activation=<span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure>
<p>这个层可以理解为一个函数，输入一个 2D 张量，返回另一个 2D 张量，即输入张量的新表示。具体而言，这个函数如下所示（其中 W 是一个 2D 张量，b 是一个向量，二者都是该层的属性）:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = relu(dot(W, input) + b)</span><br></pre></td></tr></table></figure>
<p>我们将上式拆开来看。这里有三个张量运算：输入张量和张量 W 之间的点积运算（dot）、得到的 2D 张量与向量 b 之间的加法运算（+）、最后的 relu 运算。relu(x) 是 max(x, 0)。</p>
<h3 id="2-3-1-逐元素运算"><a href="#2-3-1-逐元素运算" class="headerlink" title="2.3.1　逐元素运算"></a>2.3.1　逐元素运算</h3><ul>
<li><p>relu 运算和加法都是<strong>逐元素（element-wise）</strong>的运算，即该运算独立地应用于张量中的每个元素，也就是说，这些运算非常适合大规模并行实现（<strong>向量化</strong>实现，这一术语来自于 1970—1990 年间<strong>向量处理器</strong>超级计算机架构）。下列代码是对逐元素 relu 运算的简单实现:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def naive_relu(x):</span><br><span class="line">	assert len(x.shape) == 2 <span class="comment"># x是一个 Numpy 的 2D 张量</span></span><br><span class="line"></span><br><span class="line">	x = x.copy() <span class="comment"># 避免覆盖输入张量</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[0]):</span><br><span class="line"> 		<span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[1]):</span><br><span class="line"> 			x[i,j] = max(x[i, j], 0)</span><br><span class="line">	<span class="built_in">return</span> x</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于加法采用同样的实现方法:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def naive_add(x, y):</span><br><span class="line">	assert len(x.shape) == 2 <span class="comment"># x 和 y 是 Numpy 的 2D 张量</span></span><br><span class="line">	assert x.shape == y.shape</span><br><span class="line"></span><br><span class="line">	x = x.copy() <span class="comment"># 避免覆盖输入张量</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[0]):</span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[1]):</span><br><span class="line">			x[i,j] += y[i, j]</span><br><span class="line">	<span class="built_in">return</span> x</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="2-3-2-广播"><a href="#2-3-2-广播" class="headerlink" title="2.3.2　广播"></a>2.3.2　广播</h3><ul>
<li>上一节 naive_add 的简单实现仅支持两个形状相同的 2D 张量相加。但在前面介绍的Dense 层中，我们将一个 2D 张量与一个向量相加。如果将两个形状不同的张量相加——没有歧义的话——较小的张量会被<strong>广播（broadcast）</strong>，以匹配较大张量的形状。广播包含以下两步：</li>
</ul>
<ol>
<li><p>向较小的张量添加轴（叫作<strong>广播轴</strong>），使其 ndim 与较大的张量相同。</p>
</li>
<li><p>将较小的张量沿着新轴重复，使其形状与较大的张量相同。</p>
</li>
</ol>
<ul>
<li></li>
</ul>

      
      <!-- reward -->
      
      <div id="reward-btn">
        打赏
      </div>
      
    </div>
    
    
      <!-- copyright -->
      
        <div class="declare">
          <ul class="post-copyright">
            <li>
              <i class="ri-copyright-line"></i>
              <strong>版权声明： </strong>
              本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
            </li>
          </ul>
        </div>
        
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://hong-king.github.io/2020/05/08/%E3%80%8Apython%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/05/09/Python%E4%B8%ADcopy%E4%B8%8Edeepcopy%E7%9A%84%E5%8C%BA%E5%88%AB/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Python中copy与deepcopy的区别
          
        </div>
      </a>
    
    
      <a href="/2020/04/29/%E8%AF%AD%E8%A8%80%E4%B9%8B%E7%BE%8E-%E6%96%B0%E5%8F%A5%E7%BE%8E%E6%96%87/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">语言之美-新句美文</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        app_id: '64kjoGIqCjjRkRPmq5iQJttE-gzGzoHsz',
        app_key: 'nnOjaYgkQvKUcFiTLNqSJSpD',
        path: window.location.pathname,
        notify: 'false',
        verify: 'false',
        avatar: 'monsterid',
        placeholder: '在这里留下你的足迹吧~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        Hong-King
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Welcome to Mr.l&#39;s World"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">博客主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">文章概览</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">文章标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">文章分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/photos">光影画廊</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about_me">有关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['倾国倾城，非花非雾，春风十里独步。', '海棠 花未眠', '此后如竟没有炬火：我便是唯一的光！'],
      startDelay: 20,
      typeSpeed: 180,
      loop: true,
      backSpeed: 60,
      showCursor: true
    });
  } catch (err) {
  }

</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>





<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>

  
<script src="/js/clickLove.js"></script>



    
  </div>
</body>

</html>